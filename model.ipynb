{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4266c2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting joblib\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Installing collected packages: joblib\n",
      "Successfully installed joblib-1.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a5ccc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# model.ipynb - Cell 1\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# model.ipynb - Cell 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from backend.ml.fraud.fraud_detection import FraudDetector \n",
    "# Assuming FraudDetector class is defined and accessible\n",
    "\n",
    "# --- 1. Define Relative File Path ---\n",
    "# The notebook is in backend/ml. We use '../../' to step up to the VeriClaim root, \n",
    "# and then access the 'data' folder.\n",
    "CSV_FILE_NAME = \"health-insurance-coverage-2019.csv\"\n",
    "DATA_PATH = os.path.join(\"..\", \"..\", \"data\", CSV_FILE_NAME) \n",
    "\n",
    "print(f\"Attempting to load data from: {os.path.abspath(DATA_PATH)}\")\n",
    "\n",
    "# --- 2. Load the Data and Define Features ---\n",
    "# We use the existing utility function from FraudDetector, \n",
    "# ensuring it loads the data needed for training.\n",
    "\n",
    "# In a real scenario, you'd ensure this function uses DATA_PATH internally.\n",
    "# For simplicity, we'll assume it handles loading its data internally \n",
    "# or use a direct manual load if the internal function isn't working:\n",
    "\n",
    "try:\n",
    "    # Option A: Use the existing function (Best if it handles paths correctly)\n",
    "    df = FraudDetector.sample_training_dataframe(n_samples=5000) \n",
    "    print(f\"Dataframe loaded using FraudDetector utility with {len(df)} samples.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error using utility: {e}. Falling back to manual CSV load...\")\n",
    "    # Option B: Manual Load (If utility fails due to path)\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Manually loaded {len(df)} samples from CSV.\")\n",
    "\n",
    "# Define the features for training\n",
    "FEATURE_COLS = [\"amount\", \"days_since_last_claim\", \"num_previous_claims\", \"patient_age\"]\n",
    "\n",
    "# Check if required columns are present (Crucial step)\n",
    "for col in FEATURE_COLS:\n",
    "    if col not in df.columns:\n",
    "        print(f\"❌ ERROR: Column '{col}' missing from loaded data. Check your CSV header!\")\n",
    "        \n",
    "print(\"\\nProceed to Cell 2 for training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc538fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\arpit\\desktop\\vericlaim\\vericlaim\\.venv\\lib\\site-packages (1.5.2)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.3.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arpit\\desktop\\vericlaim\\vericlaim\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arpit\\desktop\\vericlaim\\vericlaim\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 5.2/11.0 MB 28.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.2/11.0 MB 25.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 23.6 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 5.2/8.7 MB 26.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 24.7 MB/s eta 0:00:00\n",
      "Using cached numpy-2.3.3-cp313-cp313-win_amd64.whl (12.8 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scipy-1.16.2-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "   ---------------------------------------- 0.0/38.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 5.2/38.5 MB 25.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 10.2/38.5 MB 24.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 15.2/38.5 MB 24.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 19.4/38.5 MB 23.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 24.6/38.5 MB 23.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 29.9/38.5 MB 24.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 34.9/38.5 MB 24.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.5/38.5 MB 23.1 MB/s eta 0:00:00\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, numpy, scipy, pandas, scikit-learn\n",
      "Successfully installed numpy-2.3.3 pandas-2.3.2 pytz-2025.2 scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas scikit-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a1554ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae47532f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'backend'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfraud\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfraud_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FraudDetector \n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler \u001b[38;5;66;03m# (Required if your FraudDetector uses scaling)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Define the path to the saved model\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'backend'"
     ]
    }
   ],
   "source": [
    "# model.ipynb - NEW Cell (Verification Test)\n",
    "\n",
    "# 1. Import libraries needed for loading\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from backend.ml.fraud.fraud_detection import FraudDetector \n",
    "from sklearn.preprocessing import StandardScaler # (Required if your FraudDetector uses scaling)\n",
    "\n",
    "# Define the path to the saved model\n",
    "MODEL_LOAD_PATH = os.path.join(os.getcwd(), \"fraud_model.joblib\") \n",
    "\n",
    "print(f\"Attempting to load model from: {MODEL_LOAD_PATH}\")\n",
    "\n",
    "try:\n",
    "    # 2. Load the trained model object\n",
    "    loaded_detector = joblib.load(MODEL_LOAD_PATH)\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "    # 3. Define a Test Case (Example: Low Risk Claim)\n",
    "    low_risk_claim = {\n",
    "        \"amount\": 500,                  # Low amount\n",
    "        \"days_since_last_claim\": 500,   # Long time since last claim\n",
    "        \"num_previous_claims\": 1,       # Low number of claims\n",
    "        \"patient_age\": 45               # Typical age\n",
    "    }\n",
    "\n",
    "    # 4. Predict the Anomaly Score for the low-risk case\n",
    "    low_risk_score = loaded_detector.predict_anomaly_single(low_risk_claim)\n",
    "    \n",
    "    print(\"\\n--- TEST CASE: LOW RISK ---\")\n",
    "    print(f\"Claim Data: {low_risk_claim}\")\n",
    "    print(f\"Anomaly Score (Closer to 0 is less fraud): {low_risk_score:.4f}\")\n",
    "\n",
    "    # 5. Define a High Risk Test Case (Example: High Risk Claim)\n",
    "    high_risk_claim = {\n",
    "        \"amount\": 95000,                # Very high amount\n",
    "        \"days_since_last_claim\": 10,    # Very recent claim\n",
    "        \"num_previous_claims\": 10,      # High number of claims\n",
    "        \"patient_age\": 22               # Less typical age/claim profile\n",
    "    }\n",
    "    \n",
    "    # 6. Predict the Anomaly Score for the high-risk case\n",
    "    high_risk_score = loaded_detector.predict_anomaly_single(high_risk_claim)\n",
    "    \n",
    "    print(\"\\n--- TEST CASE: HIGH RISK ---\")\n",
    "    print(f\"Claim Data: {high_risk_claim}\")\n",
    "    print(f\"Anomaly Score (Closer to 1 is high fraud risk): {high_risk_score:.4f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Model file not found at {MODEL_LOAD_PATH}. Did you run the saving cell?\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR during prediction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c1db25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Project root added to PYTHONPATH: c:\\Users\\arpit\\Desktop\\VeriClaim\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'backend'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Project root added to PYTHONPATH:\u001b[39m\u001b[33m\"\u001b[39m, project_root)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Now import your detector\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfraud\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfraud_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FraudDetector \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Define the path to the saved model\u001b[39;00m\n\u001b[32m     21\u001b[39m MODEL_LOAD_PATH = os.path.join(project_root, \u001b[33m\"\u001b[39m\u001b[33mfraud_model.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'backend'"
     ]
    }
   ],
   "source": [
    "# model.ipynb - NEW Cell (Verification Test)\n",
    "\n",
    "# 1. Import libraries needed for loading\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler  # (Required if your FraudDetector uses scaling)\n",
    "\n",
    "# --- Add project root to sys.path so Python can find \"backend\" ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(\"✅ Project root added to PYTHONPATH:\", project_root)\n",
    "\n",
    "# Now import your detector\n",
    "from backend.ml.fraud.fraud_detection import FraudDetector \n",
    "\n",
    "# Define the path to the saved model\n",
    "MODEL_LOAD_PATH = os.path.join(project_root, \"fraud_model.joblib\")\n",
    "\n",
    "print(f\"Attempting to load model from: {MODEL_LOAD_PATH}\")\n",
    "\n",
    "try:\n",
    "    # 2. Load the trained model object\n",
    "    loaded_detector = joblib.load(MODEL_LOAD_PATH)\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "    # 3. Define a Test Case (Example: Low Risk Claim)\n",
    "    low_risk_claim = {\n",
    "        \"amount\": 500,                  # Low amount\n",
    "        \"days_since_last_claim\": 500,   # Long time since last claim\n",
    "        \"num_previous_claims\": 1,       # Low number of claims\n",
    "        \"patient_age\": 45               # Typical age\n",
    "    }\n",
    "\n",
    "    # 4. Predict the Anomaly Score for the low-risk case\n",
    "    low_risk_score = loaded_detector.predict_anomaly_single(low_risk_claim)\n",
    "    \n",
    "    print(\"\\n--- TEST CASE: LOW RISK ---\")\n",
    "    print(f\"Claim Data: {low_risk_claim}\")\n",
    "    print(f\"Anomaly Score (Closer to 0 is less fraud): {low_risk_score:.4f}\")\n",
    "\n",
    "    # 5. Define a High Risk Test Case (Example: High Risk Claim)\n",
    "    high_risk_claim = {\n",
    "        \"amount\": 95000,                # Very high amount\n",
    "        \"days_since_last_claim\": 10,    # Very recent claim\n",
    "        \"num_previous_claims\": 10,      # High number of claims\n",
    "        \"patient_age\": 22               # Less typical age/claim profile\n",
    "    }\n",
    "    \n",
    "    # 6. Predict the Anomaly Score for the high-risk case\n",
    "    high_risk_score = loaded_detector.predict_anomaly_single(high_risk_claim)\n",
    "    \n",
    "    print(\"\\n--- TEST CASE: HIGH RISK ---\")\n",
    "    print(f\"Claim Data: {high_risk_claim}\")\n",
    "    print(f\"Anomaly Score (Closer to 1 is high fraud risk): {high_risk_score:.4f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Model file not found at {MODEL_LOAD_PATH}. Did you run the saving cell?\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR during prediction: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.ipynb - NEW Cell (Verification Test)\n",
    "\n",
    "# 1. Import libraries needed for loading\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler  # (Required if your FraudDetector uses scaling)\n",
    "\n",
    "# --- Add project root to sys.path so Python can find \"backend\" ---\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(\"✅ Project root added to PYTHONPATH:\", project_root)\n",
    "\n",
    "# Now import your detector\n",
    "from backend.ml.fraud.fraud_detection import FraudDetector \n",
    "\n",
    "# Define the path to the saved model\n",
    "MODEL_LOAD_PATH = os.path.join(project_root, \"fraud_model.joblib\")\n",
    "\n",
    "print(f\"Attempting to load model from: {MODEL_LOAD_PATH}\")\n",
    "\n",
    "try:\n",
    "    # 2. Load the trained model object\n",
    "    loaded_detector = joblib.load(MODEL_LOAD_PATH)\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "    # 3. Define a Test Case (Example: Low Risk Claim)\n",
    "    low_risk_claim = {\n",
    "        \"amount\": 500,                  # Low amount\n",
    "        \"days_since_last_claim\": 500,   # Long time since last claim\n",
    "        \"num_previous_claims\": 1,       # Low number of claims\n",
    "        \"patient_age\": 45               # Typical age\n",
    "    }\n",
    "\n",
    "    # 4. Predict the Anomaly Score for the low-risk case\n",
    "    low_risk_score = loaded_detector.predict_anomaly_single(low_risk_claim)\n",
    "    \n",
    "    print(\"\\n--- TEST CASE: LOW RISK ---\")\n",
    "    print(f\"Claim Data: {low_risk_claim}\")\n",
    "    print(f\"Anomaly Score (Closer to 0 is less fraud): {low_risk_score:.4f}\")\n",
    "\n",
    "    # 5. Define a High Risk Test Case (Example: High Risk Claim)\n",
    "    high_risk_claim = {\n",
    "        \"amount\": 95000,                # Very high amount\n",
    "        \"days_since_last_claim\": 10,    # Very recent claim\n",
    "        \"num_previous_claims\": 10,      # High number of claims\n",
    "        \"patient_age\": 22               # Less typical age/claim profile\n",
    "    }\n",
    "    \n",
    "    # 6. Predict the Anomaly Score for the high-risk case\n",
    "    high_risk_score = loaded_detector.predict_anomaly_single(high_risk_claim)\n",
    "    \n",
    "    print(\"\\n--- TEST CASE: HIGH RISK ---\")\n",
    "    print(f\"Claim Data: {high_risk_claim}\")\n",
    "    print(f\"Anomaly Score (Closer to 1 is high fraud risk): {high_risk_score:.4f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Model file not found at {MODEL_LOAD_PATH}. Did you run the saving cell?\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR during prediction: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2d12d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Project root added to PYTHONPATH: c:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\n",
      "Attempting to load model from: c:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\fraud_model.joblib\n",
      "❌ ERROR: Model file not found at c:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\fraud_model.joblib. Did you run the saving cell?\n"
     ]
    }
   ],
   "source": [
    "# model.ipynb - NEW Cell (Verification Test)\n",
    "\n",
    "# 1. Import libraries needed for loading\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Add project root to sys.path so Python can find \"backend\" ---\n",
    "project_root = os.path.abspath(os.getcwd())  # since notebook is now in VERICLAIM/\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(\"✅ Project root added to PYTHONPATH:\", project_root)\n",
    "\n",
    "# Now import your detector\n",
    "from backend.ml.fraud.fraud_detection import FraudDetector \n",
    "\n",
    "# Define the path to the saved model\n",
    "MODEL_LOAD_PATH = os.path.join(project_root, \"fraud_model.joblib\")\n",
    "\n",
    "print(f\"Attempting to load model from: {MODEL_LOAD_PATH}\")\n",
    "\n",
    "try:\n",
    "    # 2. Load the trained model object\n",
    "    loaded_detector = joblib.load(MODEL_LOAD_PATH)\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "    # 3. Define a Test Case (Example: Low Risk Claim)\n",
    "    low_risk_claim = {\n",
    "        \"amount\": 500,\n",
    "        \"days_since_last_claim\": 500,\n",
    "        \"num_previous_claims\": 1,\n",
    "        \"patient_age\": 45\n",
    "    }\n",
    "\n",
    "    # 4. Predict the Anomaly Score for the low-risk case\n",
    "    low_risk_score = loaded_detector.predict_anomaly_single(low_risk_claim)\n",
    "\n",
    "    print(\"\\n--- TEST CASE: LOW RISK ---\")\n",
    "    print(f\"Claim Data: {low_risk_claim}\")\n",
    "    print(f\"Anomaly Score (Closer to 0 is less fraud): {low_risk_score:.4f}\")\n",
    "\n",
    "    # 5. Define a High Risk Test Case (Example: High Risk Claim)\n",
    "    high_risk_claim = {\n",
    "        \"amount\": 95000,\n",
    "        \"days_since_last_claim\": 10,\n",
    "        \"num_previous_claims\": 10,\n",
    "        \"patient_age\": 22\n",
    "    }\n",
    "\n",
    "    # 6. Predict the Anomaly Score for the high-risk case\n",
    "    high_risk_score = loaded_detector.predict_anomaly_single(high_risk_claim)\n",
    "\n",
    "    print(\"\\n--- TEST CASE: HIGH RISK ---\")\n",
    "    print(f\"Claim Data: {high_risk_claim}\")\n",
    "    print(f\"Anomaly Score (Closer to 1 is high fraud risk): {high_risk_score:.4f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Model file not found at {MODEL_LOAD_PATH}. Did you run the saving cell?\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR during prediction: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe46ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Project root added to PYTHONPATH: c:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\n",
      "✅ Model loaded successfully from: c:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\backend\\ml\\fraud\\isolation_forest.joblib\n",
      "\n",
      "--- TEST CASE: LOW RISK ---\n",
      "Claim Data: {'amount': 500, 'days_since_last_claim': 500, 'num_previous_claims': 1, 'patient_age': 45}\n",
      "Prediction: {'is_anomaly': False, 'score': 0.1470987103307776}\n",
      "\n",
      "--- TEST CASE: HIGH RISK ---\n",
      "Claim Data: {'amount': 95000, 'days_since_last_claim': 10, 'num_previous_claims': 10, 'patient_age': 22}\n",
      "Prediction: {'is_anomaly': True, 'score': -0.14801755710314712}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\.venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator ExtraTreeRegressor from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\.venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator IsolationForest from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model.ipynb - Verification Test\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# --- Add project root to sys.path so Python can find \"backend\" ---\n",
    "project_root = os.path.abspath(os.getcwd())  # since notebook is at VERICLAIM/\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(\"✅ Project root added to PYTHONPATH:\", project_root)\n",
    "\n",
    "# Import FraudDetector\n",
    "from backend.ml.fraud.fraud_detection import FraudDetector, MODEL_PATH\n",
    "\n",
    "# Initialize FraudDetector and load the trained model\n",
    "detector = FraudDetector()\n",
    "try:\n",
    "    detector.load()\n",
    "    print(f\"✅ Model loaded successfully from: {MODEL_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Model not found at {MODEL_PATH}. Training a new one...\")\n",
    "    # create synthetic training data if not trained yet\n",
    "    df = FraudDetector.sample_training_dataframe(2000)\n",
    "    feature_cols = [\"amount\", \"days_since_last_claim\", \"num_previous_claims\", \"patient_age\"]\n",
    "    detector.fit(df, feature_cols)\n",
    "    print(f\"✅ Model trained and saved to: {MODEL_PATH}\")\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = [\"amount\", \"days_since_last_claim\", \"num_previous_claims\", \"patient_age\"]\n",
    "\n",
    "# --- Test Case: Low Risk ---\n",
    "low_risk_claim = {\n",
    "    \"amount\": 500,\n",
    "    \"days_since_last_claim\": 500,\n",
    "    \"num_previous_claims\": 1,\n",
    "    \"patient_age\": 45\n",
    "}\n",
    "\n",
    "low_risk_result = detector.predict_anomaly(low_risk_claim, feature_cols)\n",
    "\n",
    "print(\"\\n--- TEST CASE: LOW RISK ---\")\n",
    "print(f\"Claim Data: {low_risk_claim}\")\n",
    "print(f\"Prediction: {low_risk_result}\")\n",
    "\n",
    "# --- Test Case: High Risk ---\n",
    "high_risk_claim = {\n",
    "    \"amount\": 95000,\n",
    "    \"days_since_last_claim\": 10,\n",
    "    \"num_previous_claims\": 10,\n",
    "    \"patient_age\": 22\n",
    "}\n",
    "\n",
    "high_risk_result = detector.predict_anomaly(high_risk_claim, feature_cols)\n",
    "\n",
    "print(\"\\n--- TEST CASE: HIGH RISK ---\")\n",
    "print(f\"Claim Data: {high_risk_claim}\")\n",
    "print(f\"Prediction: {high_risk_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6cb52c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808517f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\.venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator ExtraTreeRegressor from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\.venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator IsolationForest from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Adjust the path if needed\n",
    "model_path = \"c:\\\\Users\\\\arpit\\\\Desktop\\\\VeriClaim\\\\VeriClaim\\\\backend\\\\ml\\\\fraud\\\\isolation_forest.joblib\"\n",
    "model = joblib.load(model_path)\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c58149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example claim data\n",
    "sample_claim = {\n",
    "    'amount': 5000,\n",
    "    'days_since_last_claim': 45,\n",
    "    'num_previous_claims': 2,\n",
    "    'policy_age': 3,\n",
    "    'claim_type': 1   # Example: 0 = minor, 1 = major\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "claim_df = pd.DataFrame([sample_claim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "330f2e1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Isolation Forest predicts -1 for anomaly (fraud) and 1 for normal\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m prediction = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m(claim_df)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrediction:\u001b[39m\u001b[33m\"\u001b[39m, prediction)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Optional: interpret results\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# Isolation Forest predicts -1 for anomaly (fraud) and 1 for normal\n",
    "prediction = model.predict(claim_df)\n",
    "print(\"Prediction:\", prediction)\n",
    "\n",
    "# Optional: interpret results\n",
    "if prediction[0] == -1:\n",
    "    print(\"⚠️ Fraudulent claim detected!\")\n",
    "else:\n",
    "    print(\"✅ Claim seems normal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2495b3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['model', 'features'])\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "print(model.keys() if isinstance(model, dict) else \"Not a dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba49dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example if it was saved in a dictionary\n",
    "if isinstance(model, dict):\n",
    "    actual_model = model['model']  # replace 'model' with the correct key if different\n",
    "else:\n",
    "    actual_model = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0042adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but IsolationForest was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 5 features, but IsolationForest is expecting 4 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m prediction = \u001b[43mactual_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclaim_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrediction:\u001b[39m\u001b[33m\"\u001b[39m, prediction)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prediction[\u001b[32m0\u001b[39m] == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_iforest.py:420\u001b[39m, in \u001b[36mIsolationForest.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    387\u001b[39m \u001b[33;03mPredict if a particular sample is an outlier or not.\u001b[39;00m\n\u001b[32m    388\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    417\u001b[39m \u001b[33;03m        model.predict(X)\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    419\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m decision_func = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m is_inlier = np.ones_like(decision_func, dtype=\u001b[38;5;28mint\u001b[39m)\n\u001b[32m    422\u001b[39m is_inlier[decision_func < \u001b[32m0\u001b[39m] = -\u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_iforest.py:474\u001b[39m, in \u001b[36mIsolationForest.decision_function\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[33;03mAverage anomaly score of X of the base classifiers.\u001b[39;00m\n\u001b[32m    428\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m \u001b[33;03m        model.decision_function(X)\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# We subtract self.offset_ to make 0 be the threshold value for being\u001b[39;00m\n\u001b[32m    472\u001b[39m \u001b[38;5;66;03m# an outlier:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscore_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m - \u001b[38;5;28mself\u001b[39m.offset_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_iforest.py:519\u001b[39m, in \u001b[36mIsolationForest.score_samples\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    478\u001b[39m \u001b[33;03mOpposite of the anomaly score defined in the original paper.\u001b[39;00m\n\u001b[32m    479\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    516\u001b[39m \u001b[33;03m        model.score(X)\u001b[39;00m\n\u001b[32m    517\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    518\u001b[39m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtree_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._score_samples(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 5 features, but IsolationForest is expecting 4 features as input."
     ]
    }
   ],
   "source": [
    "prediction = actual_model.predict(claim_df)\n",
    "print(\"Prediction:\", prediction)\n",
    "\n",
    "if prediction[0] == -1:\n",
    "    print(\"⚠️ Fraudulent claim detected!\")\n",
    "else:\n",
    "    print(\"✅ Claim seems normal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a990a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(actual_model.n_features_in_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d25983d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount', 'days_since_last_claim', 'num_previous_claims', 'policy_age']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['amount', 'days_since_last_claim', 'num_previous_claims', 'policy_age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14f8535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_claim = {\n",
    "    'amount': 5000,\n",
    "    'days_since_last_claim': 45,\n",
    "    'num_previous_claims': 2,\n",
    "    'policy_age': 3\n",
    "}\n",
    "\n",
    "claim_df = pd.DataFrame([sample_claim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb6305e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [1]\n",
      "✅ Claim seems normal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but IsolationForest was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prediction = actual_model.predict(claim_df)\n",
    "print(\"Prediction:\", prediction)\n",
    "\n",
    "if prediction[0] == -1:\n",
    "    print(\"⚠️ Fraudulent claim detected!\")\n",
    "else:\n",
    "    print(\"✅ Claim seems normal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d852c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features your model was trained on\n",
    "FEATURE_ORDER = ['amount', 'days_since_last_claim', 'num_previous_claims', 'policy_age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bc6d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example input with columns in any order or extra columns\n",
    "input_claim = {\n",
    "    'policy_age': 3,\n",
    "    'claim_type': 1,  # extra column\n",
    "    'num_previous_claims': 2,\n",
    "    'days_since_last_claim': 45,\n",
    "    'amount': 5000\n",
    "}\n",
    "\n",
    "claim_df = pd.DataFrame([input_claim])\n",
    "\n",
    "# Keep only needed columns in correct order\n",
    "claim_df = claim_df[FEATURE_ORDER]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2fff9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [1]\n",
      "✅ Claim seems normal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arpit\\Desktop\\VeriClaim\\VeriClaim\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but IsolationForest was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prediction = actual_model.predict(claim_df)\n",
    "print(\"Prediction:\", prediction)\n",
    "\n",
    "if prediction[0] == -1:\n",
    "    print(\"⚠️ Fraudulent claim detected!\")\n",
    "else:\n",
    "    print(\"✅ Claim seems normal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d2f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
